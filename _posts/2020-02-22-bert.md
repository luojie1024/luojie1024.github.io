# 介绍

> bert模型是谷歌2018年10月底公布的，反响巨大，效果不错，在各大比赛上面出类拔萃，它的提出主要是针对word2vec等模型的不足，在之前的预训练模型（包括word2vec，ELMo等）都会生成词向量，这种类别的预训练模型属于domain transfer。而近一两年提出的ULMFiT，GPT，BERT等都属于模型迁移，说白了BERT 模型是将预训练模型和下游任务模型结合在一起的，核心目的就是：是把下游具体NLP任务的活逐渐移到预训练产生词向量上。

# 资料
**1. 官方源码**
https://github.com/google-research/bert
**2. Service 实现**
https://github.com/hanxiao/bert-as-service
**3. Paper**
https://arxiv.org/abs/1810.04805
**4. 参考博文：**
https://www.cnblogs.com/rucwxb/p/10277217.html

# 介绍

> bert模型是谷歌2018年10月底公布的，反响巨大，效果不错，在各大比赛上面出类拔萃，它的提出主要是针对word2vec等模型的不足，在之前的预训练模型（包括word2vec，ELMo等）都会生成词向量，这种类别的预训练模型属于domain transfer。而近一两年提出的ULMFiT，GPT，BERT等都属于模型迁移，说白了BERT 模型是将预训练模型和下游任务模型结合在一起的，核心目的就是：是把下游具体NLP任务的活逐渐移到预训练产生词向量上。

# 资料
**1. 官方源码**
https://github.com/google-research/bert
**2. Service 实现**
https://github.com/hanxiao/bert-as-service
**3. Paper**
https://arxiv.org/abs/1810.04805
**4. 参考博文：**
https://www.cnblogs.com/rucwxb/p/10277217.html


# 关键内容

## 1. 双向Transformers
![-w1542](http://roger-markdown.oss-cn-beijing.aliyuncs.com/2020/02/22/15823579578682.jpg)
> 正如论文中所讲，目前的主要限制是当前模型不能同时考虑上下文，像上图的GPT只是一个从左到右，ELMo虽然有考虑从左到右和从右到左，但是是两个分开的网络，只有BERT是真真意义上的同时考虑了上下文

## 2. 句子级别的应用
![-w1227](http://roger-markdown.oss-cn-beijing.aliyuncs.com/2020/02/22/15823581207519.jpg)
> 通过使用segment同时考虑了句子级别的预测，具体下面实践会看到其具体是怎么做的
